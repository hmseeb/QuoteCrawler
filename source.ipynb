{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "html = requests.get('https://www.brainyquote.com/topics')\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = soup.find_all(\"a\", class_=\"topicIndexChicklet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "urls = []\n",
    "for category in categories:\n",
    "    title = category.find(\"span\", class_=\"topicContentName\").text.strip()\n",
    "    url = 'https://www.brainyquote.com' + category['href']\n",
    "    titles.append(title)\n",
    "    urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = []\n",
    "quotations = []\n",
    "index = 1\n",
    "for url in urls:\n",
    "    try:\n",
    "        html = requests.get(url, timeout=10)\n",
    "    except requests.exceptions.Timeout:\n",
    "        print('Request timed out ‚è≥')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print('Request failed ‚ùå', e)\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "    quotes = soup.find_all(\"div\", class_=\"clearfix\")\n",
    "    for quote in quotes:\n",
    "        author = quote.find(\"a\", class_=\"bq-aut\")\n",
    "        quote = quote.find(\"div\", class_=\"\")\n",
    "        authors.append(author.text.strip())\n",
    "        quotations.append(quote.text.strip())\n",
    "    print(f'Scrapped {url} üòä')\n",
    "    pagination_counter = 16\n",
    "    while (1):\n",
    "        pagination_url = url + '_' + str(pagination_counter)\n",
    "        try:\n",
    "            html = requests.get(pagination_url, timeout= 10)\n",
    "        except requests.exceptions.Timeout:\n",
    "            print('Request timed out ‚è≥')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print('Request failed ‚ùå', e)\n",
    "        if (html.url == url):\n",
    "            break\n",
    "        soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "        quotes = soup.find_all(\"div\", class_=\"clearfix\")\n",
    "        for quote in quotes:\n",
    "            author = quote.find(\"a\", class_=\"bq-aut\")\n",
    "            quote = quote.find(\"div\", class_=\"\")\n",
    "            authors.append(author.text.strip())\n",
    "            quotations.append(quote.text.strip())\n",
    "        print(f'Scrapped {pagination_url} üòä')\n",
    "        print(quotations)\n",
    "        print(authors)\n",
    "        pagination_counter += 1\n",
    "    index+=1\n",
    "    progress = index / len(urls) * 100\n",
    "    print(f'{progress:.2f}% completed üí™')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quote, auther, in zip(quotations, authers):\n",
    "      print(f\"{quote} - {auther}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
